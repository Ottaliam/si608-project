#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
Analyze vulnerability and resilience of the PyPI dependency network.

Performs:
  - Cascade failure simulations (targeted vs random attacks)
  - Component vulnerability analysis
  - Dependency depth analysis
  - Redundancy analysis
  - Critical node identification
  - Safety-DB vulnerability integration (if safety-db package is available):
    * Vulnerability-aware critical node identification
    * Vulnerability-based attack simulation
    * Risk scoring (combining centrality + vulnerabilities)
    * Vulnerability propagation analysis

Usage:
  python vulnerability_analysis.py --artifacts pypi_graph_artifacts --out vulnerability_results
  python vulnerability_analysis.py --artifacts pypi_graph_artifacts --out vulnerability_results --vulnerability-data data/insecure.json
"""

import argparse 
from pathlib import Path
import sys
import random
import numpy as np
import igraph as ig
import polars as pl
import matplotlib.pyplot as plt
import matplotlib.ticker as mtick

try:
    import seaborn as sns
    HAVE_SEABORN = True
except Exception:
    HAVE_SEABORN = False

import json

# Load vulnerability data from local JSON file
INSECURE_FULL = {}
HAVE_SAFETY_DB = False

def load_vulnerability_data(json_path: Path):
    """Load vulnerability data from insecure.json file."""
    global INSECURE_FULL, HAVE_SAFETY_DB
    try:
        if json_path.exists():
            with json_path.open("r", encoding="utf-8") as f:
                data = json.load(f)
            # Remove metadata key if present
            if "$meta" in data:
                del data["$meta"]
            INSECURE_FULL = data
            HAVE_SAFETY_DB = True
            return True
        else:
            eprint(f"Warning: Vulnerability data file not found: {json_path}")
            return False
    except Exception as e:
        eprint(f"Warning: Failed to load vulnerability data: {e}")
        return False


# -------------------------
# Helpers
# -------------------------
def eprint(*args, **kwargs):
    print(*args, file=sys.stderr, **kwargs)

def load_graph_from_ncol(ncol_path: Path):
    """Load graph from NCOL file."""
    return ig.Graph.Read_Ncol(ncol_path.as_posix(), directed=True, names=True)

def load_metric_series(path: Path, value_col: str) -> np.ndarray:
    """Load a metric series from CSV."""
    if not path.exists():
        return None
    df = pl.read_csv(path)
    return df[value_col].to_numpy()

def load_top_table(path: Path):
    """Load top-N table."""
    if not path.exists():
        return None
    return pl.read_csv(path)

def get_vertex_names(g: ig.Graph):
    """Get vertex names (project IDs)."""
    return g.vs["name"]

def get_vertex_project_names(g: ig.Graph):
    """Get vertex project names if available, otherwise return project IDs."""
    if "project_name" in g.vs.attributes():
        project_names = g.vs["project_name"]
        # Use project_name if available, fallback to project_id
        return [name if name else g.vs[vid]["name"] for vid, name in enumerate(project_names)]
    else:
        # Fallback: try to load from node_id_map.csv
        return None


# -------------------------
# Cascade Failure Simulation
# -------------------------
def simulate_cascade_failure(g: ig.Graph, nodes_to_remove: list, mode="WEAK"):
    """
    Simulate cascade failure by removing nodes and measuring impact.
    
    Args:
        g: The graph
        nodes_to_remove: List of vertex indices to remove
        mode: "WEAK" or "STRONG" for component analysis
    
    Returns:
        dict with metrics about the failure
    """
    g_copy = g.copy()
    g_copy.delete_vertices(nodes_to_remove)
    
    if g_copy.vcount() == 0:
        return {
            "nodes_remaining": 0,
            "edges_remaining": 0,
            "giant_component_size": 0,
            "num_components": 0,
            "fraction_in_giant": 0.0,
            "fraction_nodes_lost": 1.0,
        }
    
    comps = g_copy.components(mode=mode)
    giant_size = comps.giant().vcount() if comps.giant().vcount() > 0 else 0
    
    return {
        "nodes_remaining": g_copy.vcount(),
        "edges_remaining": g_copy.ecount(),
        "giant_component_size": giant_size,
        "num_components": len(comps),
        "fraction_in_giant": giant_size / g_copy.vcount() if g_copy.vcount() > 0 else 0.0,
        "fraction_nodes_lost": len(nodes_to_remove) / g.vcount(),
    }

def robustness_analysis(g: ig.Graph, metric_values: np.ndarray, metric_name: str, 
                       max_removals: int = 1000, step: int = 10, n_trials: int = 5):
    """
    Analyze robustness by removing nodes in order of metric value.
    
    Returns:
        List of dicts with removal statistics at each step
    """
    # Sort nodes by metric (descending - remove most important first)
    sorted_indices = np.argsort(metric_values)[::-1]
    
    results = []
    for k in range(0, min(max_removals, len(sorted_indices)), step):
        nodes_to_remove = sorted_indices[:k].tolist()
        if not nodes_to_remove:
            # Baseline: no removals
            comps = g.components(mode="WEAK")
            giant_size = comps.giant().vcount()
            results.append({
                "removed": 0,
                "fraction_removed": 0.0,
                "giant_component_size": giant_size,
                "fraction_in_giant": giant_size / g.vcount() if g.vcount() > 0 else 0.0,
                "num_components": len(comps),
            })
        else:
            result = simulate_cascade_failure(g, nodes_to_remove, mode="WEAK")
            results.append({
                "removed": k,
                "fraction_removed": k / g.vcount(),
                "giant_component_size": result["giant_component_size"],
                "fraction_in_giant": result["fraction_in_giant"],
                "num_components": result["num_components"],
            })
    
    # Random baseline
    random_results = []
    for k in range(0, min(max_removals, g.vcount()), step):
        if k == 0:
            comps = g.components(mode="WEAK")
            giant_size = comps.giant().vcount()
            random_results.append({
                "removed": 0,
                "fraction_removed": 0.0,
                "giant_component_size": giant_size,
                "fraction_in_giant": giant_size / g.vcount() if g.vcount() > 0 else 0.0,
                "num_components": len(comps),
            })
        else:
            # Average over n_trials random removals
            trial_giants = []
            trial_fractions = []
            trial_comps = []
            for _ in range(n_trials):
                random_nodes = random.sample(range(g.vcount()), k)
                result = simulate_cascade_failure(g, random_nodes, mode="WEAK")
                trial_giants.append(result["giant_component_size"])
                trial_fractions.append(result["fraction_in_giant"])
                trial_comps.append(result["num_components"])
            
            random_results.append({
                "removed": k,
                "fraction_removed": k / g.vcount(),
                "giant_component_size": np.mean(trial_giants),
                "fraction_in_giant": np.mean(trial_fractions),
                "num_components": np.mean(trial_comps),
            })
    
    return {
        "targeted": results,
        "random": random_results,
        "metric_name": metric_name,
    }


# -------------------------
# Component Analysis
# -------------------------
def analyze_components(g: ig.Graph):
    """Analyze weakly connected components."""
    comps = g.components(mode="WEAK")
    sizes = [len(c) for c in comps]
    sizes.sort(reverse=True)
    
    return {
        "num_components": len(comps),
        "giant_component_size": sizes[0] if sizes else 0,
        "component_sizes": sizes,
        "fraction_in_giant": sizes[0] / g.vcount() if sizes and g.vcount() > 0 else 0.0,
    }


# -------------------------
# Dependency Depth Analysis
# -------------------------
def compute_max_depth(g: ig.Graph, vertex_idx: int):
    """
    Compute maximum dependency depth (longest path) from a vertex.
    Uses BFS to find longest path in DAG.
    """
    # For directed acyclic graphs, we can use topological sort + dynamic programming
    # But igraph doesn't guarantee DAG, so we'll use a simpler approach:
    # Find longest path from this vertex using BFS
    
    visited = set()
    queue = [(vertex_idx, 0)]
    max_depth = 0
    
    while queue:
        v, depth = queue.pop(0)
        if v in visited:
            continue
        visited.add(v)
        max_depth = max(max_depth, depth)
        
        # Add neighbors (dependencies)
        for neighbor in g.successors(v):
            if neighbor not in visited:
                queue.append((neighbor, depth + 1))
    
    return max_depth

def analyze_dependency_depths(g: ig.Graph, sample_size: int = None):
    """
    Analyze dependency depths. If sample_size is None, analyze all nodes.
    """
    n = g.vcount()
    if sample_size and sample_size < n:
        indices = random.sample(range(n), sample_size)
    else:
        indices = list(range(n))
    
    depths = []
    for i, vid in enumerate(indices):
        if (i + 1) % 1000 == 0:
            eprint(f"  Computing depth for vertex {i+1}/{len(indices)}...")
        depths.append(compute_max_depth(g, vid))
    
    return {
        "depths": depths,
        "max_depth": max(depths) if depths else 0,
        "mean_depth": np.mean(depths) if depths else 0.0,
        "median_depth": np.median(depths) if depths else 0.0,
        "sampled_indices": indices if sample_size else None,
    }


# -------------------------
# Redundancy Analysis
# -------------------------
def count_dependency_paths(g: ig.Graph, source_idx: int, target_idx: int, max_depth: int = 10):
    """
    Count number of distinct paths from source to target (redundancy measure).
    Uses DFS with depth limit.
    """
    if source_idx == target_idx:
        return 1
    
    def dfs(v, target, depth, visited):
        if depth > max_depth:
            return 0
        if v == target:
            return 1
        
        count = 0
        for neighbor in g.successors(v):
            if neighbor not in visited:
                visited.add(neighbor)
                count += dfs(neighbor, target, depth + 1, visited)
                visited.remove(neighbor)
        return count
    
    return dfs(source_idx, target_idx, 0, {source_idx})

def analyze_redundancy(g: ig.Graph, top_k: int = 100):
    """
    Analyze redundancy for top-k packages by in-degree.
    For each, find packages that depend on it and check path redundancy.
    """
    indeg = g.indegree()
    top_indices = np.argsort(indeg)[-top_k:][::-1]
    
    redundancy_results = []
    for i, vid in enumerate(top_indices):
        if (i + 1) % 10 == 0:
            eprint(f"  Analyzing redundancy for top package {i+1}/{top_k}...")
        
        # Find all packages that depend on this one (predecessors in dependency graph)
        dependents = list(g.predecessors(vid))
        
        if not dependents:
            continue
        
        # Sample some dependents to analyze paths
        sample_size = min(50, len(dependents))
        sampled_dependents = random.sample(dependents, sample_size)
        
        path_counts = []
        for dep in sampled_dependents:
            # Count paths from dependent to this package
            # Note: In dependency graph, edge is (project -> dependency)
            # So we want paths from dep to vid
            paths = count_dependency_paths(g, dep, vid, max_depth=5)
            path_counts.append(paths)
        
        redundancy_results.append({
            "vertex_id": vid,
            "project_id": g.vs[vid]["name"],
            "in_degree": indeg[vid],
            "num_dependents": len(dependents),
            "mean_paths_per_dependent": np.mean(path_counts) if path_counts else 0.0,
            "fraction_multiple_paths": sum(1 for p in path_counts if p > 1) / len(path_counts) if path_counts else 0.0,
        })
    
    return redundancy_results


# -------------------------
# Critical Node Identification
# -------------------------
def identify_critical_nodes(g: ig.Graph, indeg: np.ndarray, pagerank: np.ndarray, 
                           betweenness: np.ndarray = None, top_n: int = 50):
    """
    Identify nodes that are critical by multiple metrics.
    """
    names = get_vertex_names(g)
    
    # Normalize metrics to [0, 1]
    indeg_norm = (indeg - indeg.min()) / (indeg.max() - indeg.min() + 1e-10)
    pr_norm = (pagerank - pagerank.min()) / (pagerank.max() - pagerank.min() + 1e-10)
    
    # Combined score
    combined_score = indeg_norm + pr_norm
    
    if betweenness is not None:
        betw_norm = (betweenness - betweenness.min()) / (betweenness.max() - betweenness.min() + 1e-10)
        combined_score += betw_norm
    
    top_indices = np.argsort(combined_score)[-top_n:][::-1]
    
    critical = []
    for vid in top_indices:
        critical.append({
            "vertex_id": vid,
            "project_id": names[vid],
            "in_degree": indeg[vid],
            "pagerank": pagerank[vid],
            "betweenness": betweenness[vid] if betweenness is not None else None,
            "combined_score": combined_score[vid],
        })
    
    return critical


# -------------------------
# Safety-DB Vulnerability Integration
# -------------------------
def enrich_critical_nodes_with_vulnerabilities(critical_nodes: list, project_names: list = None):
    """
    Enrich critical nodes with vulnerability information from insecure.json.
    
    Args:
        critical_nodes: List of critical node dicts with 'vertex_id' and 'project_id'
        project_names: Optional list mapping vertex_id to project_name
    
    Returns:
        List of critical nodes with added vulnerability fields and project_name
    """
    enriched = []
    for node in critical_nodes:
        # Add project name if available
        if project_names and node['vertex_id'] < len(project_names):
            node['project_name'] = project_names[node['vertex_id']] or ''
        else:
            node['project_name'] = ''
        
        # Use project_name if available, otherwise fallback to project_id
        if project_names and node['vertex_id'] < len(project_names) and project_names[node['vertex_id']]:
            lookup_key = project_names[node['vertex_id']]
        else:
            lookup_key = node['project_id']
        
        # Add vulnerability information if safety-db is available
        if HAVE_SAFETY_DB:
            # Skip if lookup_key is empty
            if not lookup_key:
                node['has_vulnerabilities'] = False
                node['num_vulnerabilities'] = 0
                enriched.append(node)
                continue
            
            # Try direct match first
            vulnerabilities = INSECURE_FULL.get(lookup_key, [])
            
            # If no direct match, try case-insensitive and normalized matching
            if not vulnerabilities:
                # Normalize: lowercase and replace common separators
                normalized_key = lookup_key.lower().replace('_', '-').replace('.', '-')
                for key, value in INSECURE_FULL.items():
                    normalized_json_key = key.lower().replace('_', '-').replace('.', '-')
                    if normalized_key == normalized_json_key:
                        vulnerabilities = value
                        break
            
            node['has_vulnerabilities'] = len(vulnerabilities) > 0
            node['num_vulnerabilities'] = len(vulnerabilities)
        else:
            node['has_vulnerabilities'] = False
            node['num_vulnerabilities'] = 0
        
        enriched.append(node)
    
    return enriched

def robustness_analysis_by_vulnerability(g: ig.Graph, max_removals: int = 1000, 
                                        step: int = 10, n_trials: int = 5,
                                        project_names: list = None):
    """
    Analyze robustness by removing vulnerable packages first.
    
    Args:
        g: The graph
        max_removals: Maximum number of nodes to remove
        step: Step size for removal
        n_trials: Number of trials for random baseline
        project_names: Optional list mapping vertex_id to project_name
    
    Returns:
        dict with vulnerability-based robustness results
    """
    if not HAVE_SAFETY_DB:
        return None
    
    names = get_vertex_names(g)
    vulnerable_indices = []
    vulnerability_counts = []
    
    # Helper function to find vulnerabilities for a project
    def find_vulnerabilities(lookup_key):
        if not lookup_key:
            return []
        # Try direct match
        vulns = INSECURE_FULL.get(lookup_key, [])
        if vulns:
            return vulns
        
        # Try case-insensitive and normalized matching
        normalized_key = lookup_key.lower().replace('_', '-').replace('.', '-')
        for key, value in INSECURE_FULL.items():
            normalized_json_key = key.lower().replace('_', '-').replace('.', '-')
            if normalized_key == normalized_json_key:
                return value
        return []
    
    # Find all vulnerable packages in the graph
    for vid in range(g.vcount()):
        # Use project_name if available, otherwise use project_id
        if project_names and vid < len(project_names) and project_names[vid]:
            lookup_key = project_names[vid]
        else:
            lookup_key = names[vid]
        
        vulns = find_vulnerabilities(lookup_key)
        if vulns:
            vuln_count = len(vulns)
            vulnerable_indices.append(vid)
            vulnerability_counts.append(vuln_count)
    
    if not vulnerable_indices:
        return None
    
    # Sort by number of vulnerabilities (most vulnerable first)
    sorted_pairs = sorted(zip(vulnerability_counts, vulnerable_indices), reverse=True)
    sorted_vulnerable = [vid for _, vid in sorted_pairs]
    
    # Perform robustness analysis
    results = []
    for k in range(0, min(max_removals, len(sorted_vulnerable)), step):
        nodes_to_remove = sorted_vulnerable[:k]
        if not nodes_to_remove:
            # Baseline: no removals
            comps = g.components(mode="WEAK")
            giant_size = comps.giant().vcount()
            results.append({
                "removed": 0,
                "fraction_removed": 0.0,
                "giant_component_size": giant_size,
                "fraction_in_giant": giant_size / g.vcount() if g.vcount() > 0 else 0.0,
                "num_components": len(comps),
            })
        else:
            result = simulate_cascade_failure(g, nodes_to_remove, mode="WEAK")
            results.append({
                "removed": k,
                "fraction_removed": k / g.vcount(),
                "giant_component_size": result["giant_component_size"],
                "fraction_in_giant": result["fraction_in_giant"],
                "num_components": result["num_components"],
            })
    
    # Random baseline
    random_results = []
    for k in range(0, min(max_removals, g.vcount()), step):
        if k == 0:
            comps = g.components(mode="WEAK")
            giant_size = comps.giant().vcount()
            random_results.append({
                "removed": 0,
                "fraction_removed": 0.0,
                "giant_component_size": giant_size,
                "fraction_in_giant": giant_size / g.vcount() if g.vcount() > 0 else 0.0,
                "num_components": len(comps),
            })
        else:
            # Average over n_trials random removals
            trial_giants = []
            trial_fractions = []
            trial_comps = []
            for _ in range(n_trials):
                random_nodes = random.sample(range(g.vcount()), k)
                result = simulate_cascade_failure(g, random_nodes, mode="WEAK")
                trial_giants.append(result["giant_component_size"])
                trial_fractions.append(result["fraction_in_giant"])
                trial_comps.append(result["num_components"])
            
            random_results.append({
                "removed": k,
                "fraction_removed": k / g.vcount(),
                "giant_component_size": np.mean(trial_giants),
                "fraction_in_giant": np.mean(trial_fractions),
                "num_components": np.mean(trial_comps),
            })
    
    return {
        "targeted": results,
        "random": random_results,
        "metric_name": "vulnerability",
        "num_vulnerable_packages": len(vulnerable_indices),
    }

def compute_risk_scores(g: ig.Graph, indeg: np.ndarray, pagerank: np.ndarray,
                       betweenness: np.ndarray = None, project_names: list = None):
    """
    Compute risk scores combining centrality metrics with vulnerability data.
    
    Args:
        g: The graph
        indeg: In-degree array
        pagerank: PageRank array
        betweenness: Optional betweenness array
        project_names: Optional list mapping vertex_id to project_name
    
    Returns:
        List of dicts with risk scores for all nodes
    """
    if not HAVE_SAFETY_DB:
        return None
    
    names = get_vertex_names(g)
    
    # Normalize centrality metrics
    indeg_norm = (indeg - indeg.min()) / (indeg.max() - indeg.min() + 1e-10)
    pr_norm = (pagerank - pagerank.min()) / (pagerank.max() - pagerank.min() + 1e-10)
    
    # Get max vulnerabilities for normalization
    max_vuln = max([len(v) for v in INSECURE_FULL.values()]) if INSECURE_FULL else 1
    
    # Helper function to find vulnerabilities for a project
    def find_vulnerabilities(lookup_key):
        if not lookup_key:
            return []
        # Try direct match
        vulns = INSECURE_FULL.get(lookup_key, [])
        if vulns:
            return vulns
        
        # Try case-insensitive and normalized matching
        normalized_key = lookup_key.lower().replace('_', '-').replace('.', '-')
        for key, value in INSECURE_FULL.items():
            normalized_json_key = key.lower().replace('_', '-').replace('.', '-')
            if normalized_key == normalized_json_key:
                return value
        return []
    
    risk_scores = []
    for vid in range(g.vcount()):
        # Use project_name if available, otherwise use project_id
        if project_names and vid < len(project_names) and project_names[vid]:
            lookup_key = project_names[vid]
        else:
            lookup_key = names[vid]
        
        # Centrality component (0-1)
        centrality_score = indeg_norm[vid] + pr_norm[vid]
        if betweenness is not None:
            betw_norm = (betweenness - betweenness.min()) / (betweenness.max() - betweenness.min() + 1e-10)
            centrality_score += betw_norm[vid]
        
        # Normalize centrality score to [0, 1] range
        # (max would be 2 or 3 depending on whether betweenness is included)
        max_centrality = 3.0 if betweenness is not None else 2.0
        centrality_score = min(centrality_score / max_centrality, 1.0)
        
        # Vulnerability component (0-1)
        vulns = find_vulnerabilities(lookup_key)
        vuln_count = len(vulns)
        vuln_score = min(vuln_count / max_vuln, 1.0) if max_vuln > 0 else 0.0
        
        # Combined risk score (weighted: 70% centrality, 30% vulnerability)
        risk_score = 0.7 * centrality_score + 0.3 * vuln_score
        
        risk_scores.append({
            "vertex_id": vid,
            "project_id": names[vid],
            "project_name": lookup_key if project_names and vid < len(project_names) else "",
            "centrality_score": centrality_score,
            "vulnerability_score": vuln_score,
            "risk_score": risk_score,
            "num_vulnerabilities": vuln_count,
            "in_degree": indeg[vid],
            "pagerank": pagerank[vid],
            "betweenness": betweenness[vid] if betweenness is not None else None,
        })
    
    return sorted(risk_scores, key=lambda x: x['risk_score'], reverse=True)

def analyze_vulnerability_propagation(g: ig.Graph, project_names: list = None):
    """
    Analyze how vulnerabilities propagate through the dependency network.
    
    Args:
        g: The graph
        project_names: Optional list mapping vertex_id to project_name
    
    Returns:
        dict with propagation statistics
    """
    if not HAVE_SAFETY_DB:
        return None
    
    names = get_vertex_names(g)
    affected_packages = set()
    vulnerable_dependencies = {}
    
    # Helper function to find vulnerabilities for a project
    def find_vulnerabilities(lookup_key):
        if not lookup_key:
            return []
        # Try direct match
        vulns = INSECURE_FULL.get(lookup_key, [])
        if vulns:
            return vulns
        
        # Try case-insensitive and normalized matching
        normalized_key = lookup_key.lower().replace('_', '-').replace('.', '-')
        for key, value in INSECURE_FULL.items():
            normalized_json_key = key.lower().replace('_', '-').replace('.', '-')
            if normalized_key == normalized_json_key:
                return value
        return []
    
    # Find all vulnerable packages
    for vid in range(g.vcount()):
        # Use project_name if available, otherwise use project_id
        if project_names and vid < len(project_names) and project_names[vid]:
            lookup_key = project_names[vid]
        else:
            lookup_key = names[vid]
        
        vulns = find_vulnerabilities(lookup_key)
        if vulns:
            vulnerable_dependencies[vid] = lookup_key
    
    if not vulnerable_dependencies:
        return {
            "total_vulnerable_packages": 0,
            "total_affected_packages": 0,
            "propagation_stats": [],
        }
    
    # For each vulnerable package, find all dependents (packages that depend on it)
    propagation_stats = []
    for vuln_vid, vuln_project_id in vulnerable_dependencies.items():
        # In dependency graph: edge (project -> dependency)
        # So predecessors depend on this vulnerable package
        dependents = list(g.predecessors(vuln_vid))
        
        affected_packages.update(dependents)
        
        # Find vulnerabilities for this package
        vulns = find_vulnerabilities(vuln_project_id)
        propagation_stats.append({
            "vulnerable_package": vuln_project_id,
            "vulnerable_vertex_id": vuln_vid,
            "num_direct_dependents": len(dependents),
            "vulnerability_count": len(vulns),
        })
    
    return {
        "total_vulnerable_packages": len(vulnerable_dependencies),
        "total_affected_packages": len(affected_packages),
        "propagation_stats": sorted(propagation_stats,
                                   key=lambda x: x['num_direct_dependents'],
                                   reverse=True),
    }


# -------------------------
# Visualization
# -------------------------
def plot_robustness_curve(out_path: Path, robustness_data: dict, title_suffix=""):
    """Plot robustness curves comparing targeted vs random attacks."""
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))
    
    targeted = robustness_data["targeted"]
    random_data = robustness_data["random"]
    metric_name = robustness_data["metric_name"]
    
    # Extract data
    frac_removed_t = [r["fraction_removed"] for r in targeted]
    frac_giant_t = [r["fraction_in_giant"] for r in targeted]
    frac_removed_r = [r["fraction_removed"] for r in random_data]
    frac_giant_r = [r["fraction_in_giant"] for r in random_data]
    
    # Plot 1: Fraction in giant component
    ax1.plot(frac_removed_t, frac_giant_t, marker="o", linestyle="-", 
             label=f"Targeted ({metric_name})", linewidth=2)
    ax1.plot(frac_removed_r, frac_giant_r, marker="s", linestyle="--", 
             label="Random", linewidth=2, alpha=0.7)
    ax1.set_xlabel("Fraction of Nodes Removed")
    ax1.set_ylabel("Fraction in Giant Component")
    ax1.set_title(f"Robustness: Giant Component Size{title_suffix}")
    ax1.legend()
    ax1.grid(True, alpha=0.3)
    ax1.set_xlim(0, max(frac_removed_t[-1], frac_removed_r[-1]))
    
    # Plot 2: Number of components
    num_comp_t = [r["num_components"] for r in targeted]
    num_comp_r = [r["num_components"] for r in random_data]
    
    ax2.plot(frac_removed_t, num_comp_t, marker="o", linestyle="-", 
             label=f"Targeted ({metric_name})", linewidth=2)
    ax2.plot(frac_removed_r, num_comp_r, marker="s", linestyle="--", 
             label="Random", linewidth=2, alpha=0.7)
    ax2.set_xlabel("Fraction of Nodes Removed")
    ax2.set_ylabel("Number of Components")
    ax2.set_title(f"Robustness: Fragmentation{title_suffix}")
    ax2.legend()
    ax2.grid(True, alpha=0.3)
    ax2.set_xlim(0, max(frac_removed_t[-1], frac_removed_r[-1]))
    
    plt.tight_layout()
    out_path.parent.mkdir(parents=True, exist_ok=True)
    fig.savefig(out_path, dpi=300, bbox_inches="tight")
    plt.close(fig)

def plot_all_robustness_curves(out_path: Path, robustness_results: dict, title_suffix=""):
    """Plot all targeted attack types on the same plot for comparison."""
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))
    
    # Define colors and markers for different attack types
    colors = {
        "in_degree": "#1f77b4",      # blue
        "pagerank": "#ff7f0e",        # orange
        "betweenness": "#2ca02c",     # green
        "vulnerability": "#d62728"    # red
    }
    markers = {
        "in_degree": "o",
        "pagerank": "s",
        "betweenness": "^",
        "vulnerability": "D"
    }
    linestyles = {
        "in_degree": "-",
        "pagerank": "--",
        "betweenness": "-.",
        "vulnerability": ":"
    }
    
    # Plot all targeted attacks
    max_x = 0
    for metric_name, robustness_data in robustness_results.items():
        if metric_name not in colors:
            continue
        
        targeted = robustness_data["targeted"]
        frac_removed_t = [r["fraction_removed"] for r in targeted]
        frac_giant_t = [r["fraction_in_giant"] for r in targeted]
        num_comp_t = [r["num_components"] for r in targeted]
        
        max_x = max(max_x, max(frac_removed_t) if frac_removed_t else 0)
        
        # Plot 1: Fraction in giant component
        ax1.plot(frac_removed_t, frac_giant_t, 
                marker=markers[metric_name], 
                linestyle=linestyles[metric_name],
                color=colors[metric_name],
                label=f"Targeted ({metric_name})", 
                linewidth=2, 
                markersize=4,
                alpha=0.8)
        
        # Plot 2: Number of components
        ax2.plot(frac_removed_t, num_comp_t,
                marker=markers[metric_name],
                linestyle=linestyles[metric_name],
                color=colors[metric_name],
                label=f"Targeted ({metric_name})",
                linewidth=2,
                markersize=4,
                alpha=0.8)
    
    # Add random baseline (use the first available random data)
    if robustness_results:
        first_metric = next(iter(robustness_results.values()))
        random_data = first_metric["random"]
        frac_removed_r = [r["fraction_removed"] for r in random_data]
        frac_giant_r = [r["fraction_in_giant"] for r in random_data]
        num_comp_r = [r["num_components"] for r in random_data]
        
        ax1.plot(frac_removed_r, frac_giant_r, 
                marker="x", linestyle="--", 
                color="gray", label="Random (baseline)", 
                linewidth=2, alpha=0.6, markersize=5)
        ax2.plot(frac_removed_r, num_comp_r,
                marker="x", linestyle="--",
                color="gray", label="Random (baseline)",
                linewidth=2, alpha=0.6, markersize=5)
    
    # Configure axes
    ax1.set_xlabel("Fraction of Nodes Removed", fontsize=11)
    ax1.set_ylabel("Fraction in Giant Component", fontsize=11)
    ax1.set_title(f"Robustness: Giant Component Size{title_suffix}", fontsize=12)
    ax1.legend(loc="best", fontsize=9)
    ax1.grid(True, alpha=0.3)
    ax1.set_xlim(0, max_x)
    
    ax2.set_xlabel("Fraction of Nodes Removed", fontsize=11)
    ax2.set_ylabel("Number of Components", fontsize=11)
    ax2.set_title(f"Robustness: Fragmentation{title_suffix}", fontsize=12)
    ax2.legend(loc="best", fontsize=9)
    ax2.grid(True, alpha=0.3)
    ax2.set_xlim(0, max_x)
    
    plt.tight_layout()
    out_path.parent.mkdir(parents=True, exist_ok=True)
    fig.savefig(out_path, dpi=300, bbox_inches="tight")
    plt.close(fig)

def plot_depth_distribution(out_path: Path, depths: list, title_suffix=""):
    """Plot distribution of dependency depths."""
    fig, ax = plt.subplots(figsize=(8, 5))
    
    ax.hist(depths, bins=50, edgecolor="black", alpha=0.7)
    ax.axvline(np.mean(depths), color="red", linestyle="--", 
               label=f"Mean: {np.mean(depths):.2f}")
    ax.axvline(np.median(depths), color="green", linestyle="--", 
               label=f"Median: {np.median(depths):.2f}")
    ax.set_xlabel("Maximum Dependency Depth")
    ax.set_ylabel("Frequency")
    ax.set_title(f"Dependency Depth Distribution{title_suffix}")
    ax.legend()
    ax.grid(True, alpha=0.3)
    
    plt.tight_layout()
    out_path.parent.mkdir(parents=True, exist_ok=True)
    fig.savefig(out_path, dpi=300, bbox_inches="tight")
    plt.close(fig)

def plot_component_size_distribution(out_path: Path, sizes: list, title_suffix=""):
    """Plot distribution of component sizes."""
    sizes_array = np.array(sizes)
    
    # Separate giant component from small components
    giant_size = sizes_array[0] if len(sizes_array) > 0 else 0
    small_components = sizes_array[1:] if len(sizes_array) > 1 else np.array([])
    
    # Create two subplots: one for small components, one for overview
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))
    
    # Left plot: Small components only (excluding giant)
    if len(small_components) > 0:
        # Use log-spaced bins for better distribution
        max_small = small_components.max()
        if max_small > 1:
            # Create log-spaced bins from 1 to max_small
            bins_small = np.logspace(0, np.log10(max_small + 1), num=30)
            bins_small = np.unique(bins_small)
            bins_small = bins_small[bins_small > 0]
            if len(bins_small) < 2:
                bins_small = np.arange(1, max_small + 2)
        else:
            bins_small = np.arange(1, max_small + 2)
        
        # Use hist directly - it handles log scale properly
        n, bins, patches = ax1.hist(small_components, bins=bins_small, edgecolor="black", alpha=0.7)
        ax1.set_xscale("log")
        ax1.set_xlabel("Component Size (log scale)")
        ax1.set_ylabel("Frequency")
        ax1.set_title(f"Small Components Distribution\n(excluding giant component of {giant_size:,} nodes)")
        ax1.grid(True, alpha=0.3, which="both")
        
        # Add statistics
        ax1.text(0.05, 0.95, f"Count: {len(small_components)}\nMax: {max_small}\nMean: {small_components.mean():.1f}\nMedian: {np.median(small_components):.1f}",
                transform=ax1.transAxes, verticalalignment="top",
                bbox=dict(boxstyle="round", facecolor="wheat", alpha=0.5))
    else:
        ax1.text(0.5, 0.5, "No small components", ha="center", va="center", transform=ax1.transAxes)
        ax1.set_title("Small Components Distribution")
    
    # Right plot: Overall distribution with log scale
    if len(sizes_array) > 0:
        # Use log-spaced bins for the full range
        min_size = sizes_array.min()
        max_size = sizes_array.max()
        
        if max_size > min_size:
            # Create log-spaced bins (keep as float for better log spacing)
            bins_full = np.logspace(np.log10(max(1, min_size)), np.log10(max_size + 1), num=50)
            bins_full = np.unique(bins_full)
            bins_full = bins_full[bins_full > 0]
            if len(bins_full) < 2:
                bins_full = np.logspace(0, np.log10(max_size + 1), num=30)
                bins_full = np.unique(bins_full)
        else:
            bins_full = [min_size, min_size + 1]
        
        # Use hist directly and color bars based on which bin contains giant component
        n, bins, patches = ax2.hist(sizes_array, bins=bins_full, edgecolor="black", alpha=0.7)
        
        # Color bars: red for bins containing/close to giant component, blue otherwise
        for i, patch in enumerate(patches):
            # Check if this bin contains or is close to the giant component
            bin_left = bins[i]
            bin_right = bins[i + 1]
            if bin_left <= giant_size <= bin_right or (bin_left <= giant_size * 1.1 <= bin_right):
                patch.set_facecolor('red')
            else:
                patch.set_facecolor('steelblue')
        
        ax2.set_xscale("log")
        ax2.set_xlabel("Component Size (log scale)")
        ax2.set_ylabel("Frequency")
        ax2.set_title(f"All Components Distribution{title_suffix}")
        ax2.grid(True, alpha=0.3, which="both")
        
        # Add annotation for giant component
        # Find the bin that contains the giant component
        giant_bin_idx = None
        for i in range(len(bins) - 1):
            if bins[i] <= giant_size <= bins[i + 1]:
                giant_bin_idx = i
                break
        
        if giant_bin_idx is not None and giant_bin_idx < len(n) and n[giant_bin_idx] > 0:
            bin_center = (bins[giant_bin_idx] + bins[giant_bin_idx + 1]) / 2
            ax2.annotate(f"Giant component\n({giant_size:,} nodes)",
                        xy=(bin_center, n[giant_bin_idx]),
                        xytext=(bin_center * 0.3, n[giant_bin_idx] * 1.1),
                        arrowprops=dict(arrowstyle="->", color="red", lw=2),
                        fontsize=9, ha="center",
                        bbox=dict(boxstyle="round", facecolor="yellow", alpha=0.7))
    else:
        ax2.text(0.5, 0.5, "No components", ha="center", va="center", transform=ax2.transAxes)
        ax2.set_title("All Components Distribution")
    
    plt.tight_layout()
    out_path.parent.mkdir(parents=True, exist_ok=True)
    fig.savefig(out_path, dpi=300, bbox_inches="tight")
    plt.close(fig)


# -------------------------
# Report Generation
# -------------------------
def write_vulnerability_report(out_path: Path, g: ig.Graph, comp_analysis: dict,
                              depth_analysis: dict, critical_nodes: list,
                              robustness_results: dict, risk_scores: list = None,
                              propagation_stats: dict = None):
    """Write comprehensive vulnerability report."""
    with out_path.open("w", encoding="utf-8") as f:
        f.write("=" * 80 + "\n")
        f.write("PYPI DEPENDENCY NETWORK VULNERABILITY ANALYSIS REPORT\n")
        f.write("=" * 80 + "\n\n")
        
        f.write("NETWORK OVERVIEW\n")
        f.write("-" * 80 + "\n")
        f.write(f"Total nodes: {g.vcount():,}\n")
        f.write(f"Total edges: {g.ecount():,}\n")
        f.write(f"Density: {g.ecount() / (g.vcount() * (g.vcount() - 1)):.6e}\n\n")
        
        f.write("COMPONENT ANALYSIS\n")
        f.write("-" * 80 + "\n")
        f.write(f"Number of weakly connected components: {comp_analysis['num_components']:,}\n")
        f.write(f"Giant component size: {comp_analysis['giant_component_size']:,}\n")
        f.write(f"Fraction of nodes in giant component: {comp_analysis['fraction_in_giant']:.4f}\n")
        f.write(f"Largest 10 components: {comp_analysis['component_sizes'][:10]}\n\n")
        
        f.write("DEPENDENCY DEPTH ANALYSIS\n")
        f.write("-" * 80 + "\n")
        f.write(f"Maximum depth: {depth_analysis['max_depth']}\n")
        f.write(f"Mean depth: {depth_analysis['mean_depth']:.2f}\n")
        f.write(f"Median depth: {depth_analysis['median_depth']:.2f}\n\n")
        
        f.write("CRITICAL NODES (Top 20)\n")
        f.write("-" * 100 + "\n")
        if critical_nodes and 'has_vulnerabilities' in critical_nodes[0]:
            f.write(f"{'Rank':<6} {'Project ID':<12} {'Project Name':<30} {'In-Degree':<12} {'PageRank':<12} {'Vulns':<8} {'Combined Score':<15}\n")
            f.write("-" * 100 + "\n")
            for i, node in enumerate(critical_nodes[:20], 1):
                vuln_count = node.get('num_vulnerabilities', 0)
                project_name = node.get('project_name', '') or ''
                # Truncate long project names
                if len(project_name) > 28:
                    project_name = project_name[:25] + "..."
                f.write(f"{i:<6} {node['project_id']:<12} {project_name:<30} {node['in_degree']:<12} "
                       f"{node['pagerank']:<12.6e} {vuln_count:<8} {node['combined_score']:<15.6f}\n")
        else:
            f.write(f"{'Rank':<6} {'Project ID':<12} {'Project Name':<30} {'In-Degree':<12} {'PageRank':<12} {'Combined Score':<15}\n")
            f.write("-" * 100 + "\n")
            for i, node in enumerate(critical_nodes[:20], 1):
                project_name = node.get('project_name', '') or ''
                # Truncate long project names
                if len(project_name) > 28:
                    project_name = project_name[:25] + "..."
                f.write(f"{i:<6} {node['project_id']:<12} {project_name:<30} {node['in_degree']:<12} "
                       f"{node['pagerank']:<12.6e} {node['combined_score']:<15.6f}\n")
        f.write("\n")
        
        # Safety-DB Vulnerability Analysis Section
        if HAVE_SAFETY_DB:
            f.write("SAFETY-DB VULNERABILITY ANALYSIS\n")
            f.write("-" * 80 + "\n")
            
            # Critical nodes with vulnerabilities
            vulnerable_critical = [n for n in critical_nodes if n.get('has_vulnerabilities', False)]
            f.write(f"Critical nodes with known vulnerabilities: {len(vulnerable_critical)}/{len(critical_nodes)}\n")
            
            if risk_scores:
                f.write(f"\nTop 20 High-Risk Packages (Centrality + Vulnerabilities):\n")
                f.write(f"{'Rank':<6} {'Project ID':<12} {'Project Name':<30} {'Risk Score':<12} {'Vulns':<8} {'Centrality':<12} {'Vuln Score':<12}\n")
                f.write("-" * 100 + "\n")
                for i, node in enumerate(risk_scores[:20], 1):
                    project_name = node.get('project_name', '') or ''
                    # Truncate long project names
                    if len(project_name) > 28:
                        project_name = project_name[:25] + "..."
                    f.write(f"{i:<6} {node['project_id']:<12} {project_name:<30} {node['risk_score']:<12.4f} "
                           f"{node['num_vulnerabilities']:<8} {node['centrality_score']:<12.4f} "
                           f"{node['vulnerability_score']:<12.4f}\n")
            
            if propagation_stats:
                f.write(f"\nVulnerability Propagation:\n")
                f.write(f"Total vulnerable packages: {propagation_stats['total_vulnerable_packages']:,}\n")
                f.write(f"Total packages affected (dependents): {propagation_stats['total_affected_packages']:,}\n")
                f.write(f"Top 10 vulnerable packages by impact:\n")
                f.write(f"{'Rank':<6} {'Package':<20} {'Dependents':<12} {'Vulnerabilities':<15}\n")
                f.write("-" * 80 + "\n")
                for i, stat in enumerate(propagation_stats['propagation_stats'][:10], 1):
                    f.write(f"{i:<6} {stat['vulnerable_package']:<20} {stat['num_direct_dependents']:<12} "
                           f"{stat['vulnerability_count']:<15}\n")
            f.write("\n")
        else:
            f.write("SAFETY-DB VULNERABILITY ANALYSIS\n")
            f.write("-" * 80 + "\n")
            f.write("Safety-DB not available. Install safety-db package to enable vulnerability analysis.\n\n")
        
        f.write("ROBUSTNESS ANALYSIS\n")
        f.write("-" * 80 + "\n")
        for metric_name, robustness_data in robustness_results.items():
            f.write(f"\nMetric: {metric_name}\n")
            targeted = robustness_data["targeted"]
            random_data = robustness_data["random"]
            
            # Find point where giant component drops below 50%
            for r in targeted:
                if r["fraction_in_giant"] < 0.5:
                    f.write(f"  Targeted attack: Giant component <50% after removing "
                           f"{r['fraction_removed']:.2%} of nodes\n")
                    break
            
            for r in random_data:
                if r["fraction_in_giant"] < 0.5:
                    f.write(f"  Random attack: Giant component <50% after removing "
                           f"{r['fraction_removed']:.2%} of nodes\n")
                    break


# -------------------------
# Main
# -------------------------
def main():
    parser = argparse.ArgumentParser(
        description="Analyze vulnerability and resilience of PyPI dependency network."
    )
    parser.add_argument("--artifacts", type=Path, required=True,
                       help="Path to pypi_graph_artifacts directory")
    parser.add_argument("--out", type=Path, default=Path("vulnerability_results"),
                       help="Output directory for results")
    parser.add_argument("--max-removals", type=int, default=1000,
                       help="Maximum number of nodes to remove in robustness analysis")
    parser.add_argument("--removal-step", type=int, default=10,
                       help="Step size for node removal")
    parser.add_argument("--depth-sample", type=int, default=None,
                       help="Sample size for depth analysis (None = all nodes)")
    parser.add_argument("--redundancy-top-k", type=int, default=50,
                       help="Top-K packages to analyze for redundancy")
    parser.add_argument("--vulnerability-data", type=Path, default=Path("data/insecure.json"),
                       help="Path to insecure.json vulnerability data file")
    parser.add_argument("--use-seaborn", action="store_true",
                       help="Use seaborn styling if available")
    
    args = parser.parse_args()
    
    if args.use_seaborn and HAVE_SEABORN:
        sns.set_theme(style="whitegrid")
    else:
        plt.style.use("default")
    
    artifacts = args.artifacts
    outdir = args.out
    outdir.mkdir(parents=True, exist_ok=True)
    
    # Load vulnerability data from JSON file
    if load_vulnerability_data(args.vulnerability_data):
        eprint(f"[ok] Loaded vulnerability data: {len(INSECURE_FULL)} packages")
    else:
        eprint("[warning] Vulnerability data not available, continuing without it")
    
    # Load graph
    eprint("[step] Loading graph from NCOL...")
    ncol_path = artifacts / "edges_pypi_intra.ncol"
    if not ncol_path.exists():
        eprint(f"ERROR: NCOL file not found: {ncol_path}")
        sys.exit(1)
    
    g = load_graph_from_ncol(ncol_path)
    eprint(f"[ok] Graph loaded: |V|={g.vcount():,} |E|={g.ecount():,}")
    
    # Load project names if available
    project_names = None
    node_id_map_path = artifacts / "node_id_map.csv"
    if node_id_map_path.exists():
        try:
            node_map_df = pl.read_csv(node_id_map_path)
            if "project_name" in node_map_df.columns:
                # Create a list mapping vertex_id to project_name
                project_names = [""] * g.vcount()
                for row in node_map_df.iter_rows(named=True):
                    vid = row["vertex_id"]
                    if vid < len(project_names):
                        project_names[vid] = row.get("project_name", "") or ""
                eprint(f"[ok] Loaded project names for {sum(1 for n in project_names if n)} vertices")
            else:
                eprint("[info] Project names not found in node_id_map.csv, using project IDs")
        except Exception as e:
            eprint(f"[warning] Failed to load project names: {e}")
    
    # Also check if project_name is stored as vertex attribute
    if "project_name" in g.vs.attributes():
        project_names = g.vs["project_name"]
        eprint(f"[ok] Using project names from graph vertex attributes")
    
    # Load existing metrics
    eprint("[step] Loading existing metrics...")
    metrics_dir = artifacts / "metrics"
    
    indeg = load_metric_series(metrics_dir / "in_degree.csv", "in_degree")
    outdeg = load_metric_series(metrics_dir / "out_degree.csv", "out_degree")
    pagerank = load_metric_series(metrics_dir / "pagerank.csv", "pagerank")
    betweenness = load_metric_series(metrics_dir / "betweenness.csv", "betweenness")
    
    if indeg is None or pagerank is None:
        eprint("ERROR: Required metrics not found. Run basic_analysis.py first.")
        sys.exit(1)
    
    eprint("[ok] Metrics loaded")
    
    # Component analysis
    eprint("[step] Analyzing components...")
    comp_analysis = analyze_components(g)
    eprint(f"[ok] Found {comp_analysis['num_components']} components, "
           f"giant component: {comp_analysis['giant_component_size']:,} nodes")
    
    # Dependency depth analysis
    eprint("[step] Analyzing dependency depths...")
    depth_analysis = analyze_dependency_depths(g, sample_size=args.depth_sample)
    eprint(f"[ok] Max depth: {depth_analysis['max_depth']}, "
           f"Mean: {depth_analysis['mean_depth']:.2f}")
    
    # Critical node identification
    eprint("[step] Identifying critical nodes...")
    critical_nodes = identify_critical_nodes(g, indeg, pagerank, betweenness, top_n=100)
    eprint(f"[ok] Identified {len(critical_nodes)} critical nodes")
    
    # Enrich critical nodes with vulnerability data and project names
    eprint("[step] Enriching critical nodes with vulnerability data and project names...")
    critical_nodes = enrich_critical_nodes_with_vulnerabilities(critical_nodes, project_names)
    if HAVE_SAFETY_DB:
        vulnerable_count = sum(1 for n in critical_nodes if n.get('has_vulnerabilities', False))
        eprint(f"[ok] Found {vulnerable_count} critical nodes with known vulnerabilities")
    else:
        eprint("[ok] Enriched critical nodes with project names")
    
    # Robustness analysis
    eprint("[step] Performing robustness analysis (this may take a while)...")
    robustness_results = {}
    
    # By in-degree
    eprint("  Analyzing robustness to in-degree attacks...")
    robustness_results["in_degree"] = robustness_analysis(
        g, indeg, "in_degree", 
        max_removals=args.max_removals, 
        step=args.removal_step
    )
    
    # By PageRank
    eprint("  Analyzing robustness to PageRank attacks...")
    robustness_results["pagerank"] = robustness_analysis(
        g, pagerank, "pagerank",
        max_removals=args.max_removals,
        step=args.removal_step
    )
    
    if betweenness is not None:
        eprint("  Analyzing robustness to betweenness attacks...")
        robustness_results["betweenness"] = robustness_analysis(
            g, betweenness, "betweenness",
            max_removals=args.max_removals,
            step=args.removal_step
        )
    
    # Vulnerability-based robustness analysis
    if HAVE_SAFETY_DB:
        eprint("  Analyzing robustness to vulnerability-based attacks...")
        vuln_robustness = robustness_analysis_by_vulnerability(
            g,
            max_removals=args.max_removals,
            step=args.removal_step,
            project_names=project_names
        )
        if vuln_robustness:
            robustness_results["vulnerability"] = vuln_robustness
            eprint(f"    Found {vuln_robustness['num_vulnerable_packages']} vulnerable packages")
    
    eprint("[ok] Robustness analysis complete")
    
    # Redundancy analysis (optional, can be slow)
    eprint("[step] Analyzing redundancy (this may take a while)...")
    redundancy_results = analyze_redundancy(g, top_k=args.redundancy_top_k)
    eprint(f"[ok] Analyzed redundancy for {len(redundancy_results)} packages")
    
    # Safety-DB vulnerability analysis
    risk_scores = None
    propagation_stats = None
    if HAVE_SAFETY_DB:
        eprint("[step] Computing risk scores (centrality + vulnerabilities)...")
        risk_scores = compute_risk_scores(g, indeg, pagerank, betweenness, project_names)
        eprint(f"[ok] Computed risk scores for {len(risk_scores)} packages")
        
        eprint("[step] Analyzing vulnerability propagation...")
        propagation_stats = analyze_vulnerability_propagation(g, project_names)
        if propagation_stats:
            eprint(f"[ok] Found {propagation_stats['total_vulnerable_packages']} vulnerable packages "
                   f"affecting {propagation_stats['total_affected_packages']} dependents")
    
    # Save results
    eprint("[step] Saving results...")
    
    # Save critical nodes
    with (outdir / "critical_nodes.csv").open("w", encoding="utf-8") as f:
        if critical_nodes and 'has_vulnerabilities' in critical_nodes[0]:
            f.write("rank,vertex_id,project_id,in_degree,pagerank,betweenness,combined_score,has_vulnerabilities,num_vulnerabilities\n")
            for i, node in enumerate(critical_nodes, 1):
                betw = node["betweenness"] if node["betweenness"] is not None else ""
                has_vuln = node.get('has_vulnerabilities', False)
                num_vuln = node.get('num_vulnerabilities', 0)
                f.write(f"{i},{node['vertex_id']},{node['project_id']},"
                       f"{node['in_degree']},{node['pagerank']},{betw},{node['combined_score']},"
                       f"{has_vuln},{num_vuln}\n")
        else:
            f.write("rank,vertex_id,project_id,in_degree,pagerank,betweenness,combined_score\n")
            for i, node in enumerate(critical_nodes, 1):
                betw = node["betweenness"] if node["betweenness"] is not None else ""
                f.write(f"{i},{node['vertex_id']},{node['project_id']},"
                       f"{node['in_degree']},{node['pagerank']},{betw},{node['combined_score']}\n")
    
    # Save risk scores
    if risk_scores:
        with (outdir / "risk_scores.csv").open("w", encoding="utf-8") as f:
            f.write("rank,vertex_id,project_id,risk_score,centrality_score,vulnerability_score,"
                   "num_vulnerabilities,in_degree,pagerank,betweenness\n")
            for i, node in enumerate(risk_scores, 1):
                betw = node["betweenness"] if node["betweenness"] is not None else ""
                f.write(f"{i},{node['vertex_id']},{node['project_id']},"
                       f"{node['risk_score']:.6f},{node['centrality_score']:.6f},"
                       f"{node['vulnerability_score']:.6f},{node['num_vulnerabilities']},"
                       f"{node['in_degree']},{node['pagerank']},{betw}\n")
    
    # Save vulnerability propagation stats
    if propagation_stats:
        with (outdir / "vulnerability_propagation.csv").open("w", encoding="utf-8") as f:
            f.write("rank,vulnerable_vertex_id,vulnerable_package,num_direct_dependents,vulnerability_count\n")
            for i, stat in enumerate(propagation_stats['propagation_stats'], 1):
                f.write(f"{i},{stat['vulnerable_vertex_id']},{stat['vulnerable_package']},"
                       f"{stat['num_direct_dependents']},{stat['vulnerability_count']}\n")
    
    # Save redundancy results
    with (outdir / "redundancy_analysis.csv").open("w", encoding="utf-8") as f:
        f.write("vertex_id,project_id,in_degree,num_dependents,mean_paths_per_dependent,"
               "fraction_multiple_paths\n")
        for r in redundancy_results:
            f.write(f"{r['vertex_id']},{r['project_id']},{r['in_degree']},"
                   f"{r['num_dependents']},{r['mean_paths_per_dependent']:.4f},"
                   f"{r['fraction_multiple_paths']:.4f}\n")
    
    # Save depth analysis
    if depth_analysis["sampled_indices"] is None:
        # Full analysis
        with (outdir / "dependency_depths.csv").open("w", encoding="utf-8") as f:
            f.write("vertex_id,depth\n")
            for vid, depth in enumerate(depth_analysis["depths"]):
                f.write(f"{vid},{depth}\n")
    else:
        # Sampled analysis
        with (outdir / "dependency_depths.csv").open("w", encoding="utf-8") as f:
            f.write("vertex_id,depth\n")
            for vid, depth in zip(depth_analysis["sampled_indices"], depth_analysis["depths"]):
                f.write(f"{vid},{depth}\n")
    
    # Generate visualizations
    eprint("[step] Generating visualizations...")
    
    # Combined robustness curves (all targeted attacks on same plot)
    if robustness_results:
        plot_all_robustness_curves(
            outdir / "robustness_all_attacks.png",
            robustness_results,
            title_suffix=f"  |V|={g.vcount():,}"
        )
        eprint("[ok] Generated combined robustness plot")
    
    # Individual robustness curves (one per metric)
    for metric_name, robustness_data in robustness_results.items():
        plot_robustness_curve(
            outdir / f"robustness_{metric_name}.png",
            robustness_data,
            title_suffix=f"  |V|={g.vcount():,}"
        )
    
    # Depth distribution
    plot_depth_distribution(
        outdir / "depth_distribution.png",
        depth_analysis["depths"],
        title_suffix=f"  |V|={g.vcount():,}"
    )
    
    # Component size distribution
    plot_component_size_distribution(
        outdir / "component_sizes.png",
        comp_analysis["component_sizes"],
        title_suffix=f"  |V|={g.vcount():,}"
    )
    
    # Generate report
    write_vulnerability_report(
        outdir / "vulnerability_report.txt",
        g, comp_analysis, depth_analysis, critical_nodes, robustness_results,
        risk_scores=risk_scores, propagation_stats=propagation_stats
    )
    
    eprint(f"[done] Results written to: {outdir.resolve()}")

if __name__ == "__main__":
    main()

